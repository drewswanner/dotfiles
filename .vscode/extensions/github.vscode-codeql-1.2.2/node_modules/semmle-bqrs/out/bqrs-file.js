"use strict";
var __await = (this && this.__await) || function (v) { return this instanceof __await ? (this.v = v, this) : new __await(v); }
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
var __asyncGenerator = (this && this.__asyncGenerator) || function (thisArg, _arguments, generator) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var g = generator.apply(thisArg, _arguments || []), i, q = [];
    return i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i;
    function verb(n) { if (g[n]) i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; }
    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }
    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }
    function fulfill(value) { resume("next", value); }
    function reject(value) { resume("throw", value); }
    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }
};
Object.defineProperty(exports, "__esModule", { value: true });
const semmle_io_1 = require("semmle-io");
const bqrs_parse_1 = require("./bqrs-parse");
/** Reads data from the specified region of the file, and parses it using the given function. */
async function inFileRegion(file, start, end, parse) {
    const stream = file.readStream(start, end);
    try {
        const d = semmle_io_1.StreamDigester.fromChunkIterator(stream);
        const result = await parse(d);
        return {
            result: result,
            finalOffset: start + d.position
        };
    }
    finally {
        stream.dispose();
    }
}
class ResultSetReaderImpl {
    constructor(resultSets, info) {
        this.resultSets = resultSets;
        this.schema = info.schema;
        this.rowsOffset = info.rowsOffset;
        this.rowsLength = info.rowsLength;
    }
    readTuples() {
        return __asyncGenerator(this, arguments, function* readTuples_1() {
            var e_1, _a;
            const stream = this.resultSets.file.readStream(this.rowsOffset, this.rowsOffset + this.rowsLength);
            try {
                const d = semmle_io_1.StreamDigester.fromChunkIterator(stream);
                try {
                    for (var _b = __asyncValues(bqrs_parse_1.readTuples(d, this.schema, yield __await(this.resultSets.getStringPool()))), _c; _c = yield __await(_b.next()), !_c.done;) {
                        const tuple = _c.value;
                        yield yield __await(tuple);
                    }
                }
                catch (e_1_1) { e_1 = { error: e_1_1 }; }
                finally {
                    try {
                        if (_c && !_c.done && (_a = _b.return)) yield __await(_a.call(_b));
                    }
                    finally { if (e_1) throw e_1.error; }
                }
            }
            finally {
                stream.dispose();
            }
        });
    }
}
class ResultSetsReaderImpl {
    constructor(file, schema, resultSets, stringPoolOffset) {
        this.file = file;
        this.schema = schema;
        this.stringPoolOffset = stringPoolOffset;
        this.stringPool = undefined;
        this._resultSets = resultSets.map((info) => {
            return new ResultSetReaderImpl(this, info);
        });
    }
    get resultSets() {
        return this._resultSets;
    }
    findResultSetByName(name) {
        return this._resultSets.find((resultSet) => resultSet.schema.name === name);
    }
    async getStringPool() {
        if (this.stringPool === undefined) {
            const { result: stringPoolBuffer } = await inFileRegion(this.file, this.stringPoolOffset, this.stringPoolOffset + this.schema.stringPoolSize, async (d) => await d.read(this.schema.stringPoolSize));
            this.stringPool = new bqrs_parse_1.StringPool(stringPoolBuffer);
        }
        return this.stringPool;
    }
    static async open(file) {
        // Parse the header of the entire BQRS file.
        const { result: header, finalOffset: stringPoolOffset } = await inFileRegion(file, 0, undefined, d => bqrs_parse_1.parseResultSetsHeader(d));
        // The header is followed by a shared string pool.
        // We have saved the offset and length of the string pool within the file,
        // so we can read it later when needed.
        // For now, skip over the string pool to reach the starting point of the first result set.
        let currentResultSetOffset = stringPoolOffset + header.stringPoolSize;
        //  Parse information about each result set within the file.
        const resultSets = [];
        for (let resultSetIndex = 0; resultSetIndex < header.resultSetCount; resultSetIndex++) {
            // Read the length of this result set (encoded as a single byte).
            // Note: reading length and schema together from a file region may be more efficient.
            // Reading them separately just makes it easier to compute the
            // starting offset and length of the schema.
            const { result: resultSetLength, finalOffset: resultSetSchemaOffset } = await inFileRegion(file, currentResultSetOffset, undefined, d => d.readLEB128UInt32());
            // Read the schema of this result set.
            const { result: resultSetSchema, finalOffset: resultSetRowsOffset } = await inFileRegion(file, resultSetSchemaOffset, undefined, d => bqrs_parse_1.parseResultSetSchema(d));
            const resultSetSchemaLength = resultSetRowsOffset - resultSetSchemaOffset;
            // The schema is followed by the tuple/row data for the result set.
            // We save the offset and length of the tuple data within the file,
            // so we can read it later when needed.
            const info = {
                // length of result set = length of schema + length of tuple data
                // The 1 byte that encodes the length itself is not counted.
                rowsLength: resultSetLength - resultSetSchemaLength,
                rowsOffset: resultSetRowsOffset,
                schema: resultSetSchema,
            };
            resultSets.push(info);
            // Skip over the tuple data of the current result set,
            // to reach the starting offset of the next result set.
            currentResultSetOffset = info.rowsOffset + info.rowsLength;
        }
        const schema = {
            version: header.version,
            stringPoolSize: header.stringPoolSize,
            resultSets: resultSets.map(resultSet => resultSet.schema)
        };
        const reader = new ResultSetsReaderImpl(file, schema, resultSets, stringPoolOffset);
        return reader;
    }
}
function open(file) {
    return ResultSetsReaderImpl.open(file);
}
exports.open = open;

//# sourceMappingURL=bqrs-file.js.map
